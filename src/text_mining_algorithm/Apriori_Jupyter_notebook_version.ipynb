{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "nLzjvZmTg0N4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mlxtend\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "dataset_filepath = '../../dataset/dataset.csv'\n",
    "dataset = pd.read_csv(dataset_filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Figs. 2 and 3\n",
    "All = dataset\n",
    "\n",
    "All_s = All[['statement']]\n",
    "\n",
    "# Step 1: Load sentences\n",
    "sentences_All = All_s['statement'].tolist()\n",
    "# Preprocessing steps here, e.g., tokenization, stopword removal, etc.\n",
    "\n",
    "# Step 2: Convert sentences to transactions\n",
    "transactions_All = []\n",
    "for sentence in sentences_All:\n",
    "    transaction_All = str(sentence).split()  # Split sentence into items (words)\n",
    "    transactions_All.append(transaction_All)\n",
    "\n",
    "# Step 3: Create transaction dataset\n",
    "te_All = TransactionEncoder()\n",
    "te_ary_All = te_All.fit_transform(transactions_All)\n",
    "transaction_df_All = pd.DataFrame(te_ary_All, columns=te_All.columns_)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W01wdPQQ3_qC",
    "outputId": "aff17cee-3623-4ddd-f404-c8127af7994a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Support guide calculation\n",
    "sentences = transactions_All\n",
    "word1 = \"kardashian\"\n",
    "word2 = \"kim\"\n",
    "\n",
    "count = 0\n",
    "for sentence in sentences:\n",
    "    if word1 in sentence and word2 in sentence:\n",
    "        count += 1\n",
    "\n",
    "print(f\"The words '{word1}' and '{word2}' appear together in {count} sentences.\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0gP2W32lTLjy",
    "outputId": "4214dc26-d726-4b17-e22c-fee28ad08ef8"
   },
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The words 'kardashian' and 'kim' appear together in 507 sentences.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(transaction_df_All.shape)"
   ],
   "metadata": {
    "id": "zIZbnuW1wbQ0"
   },
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35987, 27884)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "frequent_itemsets_All = apriori(transaction_df_All, min_support=0.004, use_colnames=True)"
   ],
   "metadata": {
    "id": "Ai4pKJYawc3c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(frequent_itemsets_All.shape)"
   ],
   "metadata": {
    "id": "VLUmLazKlxIu"
   },
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(482, 2)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "rules_All = association_rules(frequent_itemsets_All, metric=\"confidence\", min_threshold=0.9)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EvbmLYmSwhMb",
    "outputId": "195c8a2c-c891-47f4-aec5-ca3bdd2aac1a"
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "selected_columns = ['antecedents', 'consequents', 'support', 'confidence']\n",
    "print(rules_All.loc[:, selected_columns])"
   ],
   "metadata": {
    "id": "Rj2ZTW8hyEpA",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "e0545a96-a7d9-4866-fd2b-22609668fda1"
   },
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                antecedents       consequents   support  confidence\n",
      "0                   (jolie)        (angelina)  0.007892    0.965986\n",
      "1                 (aniston)        (jennifer)  0.007864    0.965870\n",
      "2                  (bieber)          (justin)  0.007419    0.978022\n",
      "3                 (shelton)           (blake)  0.005307    0.989637\n",
      "4                    (pitt)            (brad)  0.010976    0.994962\n",
      "5                  (carpet)             (red)  0.005780    0.985782\n",
      "6                  (disick)           (scott)  0.004085    0.960784\n",
      "7                   (gomez)          (selena)  0.010198    0.991892\n",
      "8                 (stefani)            (gwen)  0.004752    0.994186\n",
      "9                   (lopez)        (jennifer)  0.004641    0.954286\n",
      "10                   (katy)           (perry)  0.004113    0.902439\n",
      "11                 (markle)          (meghan)  0.010754    0.992308\n",
      "12                   (york)             (new)  0.004585    1.000000\n",
      "13                 (united)          (states)  0.010365    0.934837\n",
      "14                  (swift)          (taylor)  0.007558    0.971429\n",
      "15            (brad, jolie)        (angelina)  0.004641    0.954286\n",
      "16         (pitt, angelina)            (brad)  0.005196    0.994681\n",
      "17         (brad, angelina)            (pitt)  0.005196    0.907767\n",
      "18            (pitt, jolie)        (angelina)  0.004224    0.993464\n",
      "19         (bieber, selena)          (justin)  0.004224    0.980645\n",
      "20            (pitt, jolie)            (brad)  0.004224    0.993464\n",
      "21          (justin, gomez)          (selena)  0.004807    1.000000\n",
      "22          (harry, markle)          (meghan)  0.005391    0.994872\n",
      "23          (harry, markle)          (prince)  0.005280    0.974359\n",
      "24       (west, kardashian)             (kim)  0.004057    0.973333\n",
      "25         (prince, markle)          (meghan)  0.005891    0.990654\n",
      "26      (pitt, brad, jolie)        (angelina)  0.004196    0.993421\n",
      "27  (pitt, angelina, jolie)            (brad)  0.004196    0.993421\n",
      "28  (brad, angelina, jolie)            (pitt)  0.004196    0.904192\n",
      "29            (pitt, jolie)  (brad, angelina)  0.004196    0.986928\n",
      "30  (harry, meghan, markle)          (prince)  0.005252    0.974227\n",
      "31  (harry, prince, markle)          (meghan)  0.005252    0.994737\n",
      "32          (harry, markle)  (meghan, prince)  0.005252    0.969231\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "antecedents_set_All = set(rules_All['antecedents'].tolist())\n",
    "consequents_set_All = set(rules_All['consequents'].tolist())\n",
    "combined_set_All = antecedents_set_All | consequents_set_All\n",
    "\n",
    "words_All = [list(word_set_All)[0] for word_set_All in combined_set_All]\n",
    "unique_words_All = set(words_All)\n",
    "print(unique_words_All)"
   ],
   "metadata": {
    "id": "dApD96OfiTdd"
   },
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'united', 'stefani', 'meghan', 'blake', 'york', 'gwen', 'new', 'selena', 'gomez', 'shelton', 'jennifer', 'red', 'brad', 'carpet', 'swift', 'kim', 'justin', 'bieber', 'prince', 'states', 'disick', 'jolie', 'taylor', 'pitt', 'west', 'lopez', 'markle', 'perry', 'angelina', 'scott', 'katy', 'harry', 'aniston'}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Sort the rules based on support in descending order\n",
    "sorted_rules = rules_All.sort_values('confidence', ascending=False)\n",
    "\n",
    "# Print the top 10 rules with highest support\n",
    "print(sorted_rules.head(20))"
   ],
   "metadata": {
    "id": "eLyFX6wniZAF"
   },
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                antecedents       consequents  antecedent support   \n",
      "21          (justin, gomez)          (selena)            0.004807  \\\n",
      "12                   (york)             (new)            0.004585   \n",
      "4                    (pitt)            (brad)            0.011032   \n",
      "22          (harry, markle)          (meghan)            0.005419   \n",
      "31  (harry, prince, markle)          (meghan)            0.005280   \n",
      "16         (pitt, angelina)            (brad)            0.005224   \n",
      "8                 (stefani)            (gwen)            0.004780   \n",
      "20            (pitt, jolie)            (brad)            0.004252   \n",
      "18            (pitt, jolie)        (angelina)            0.004252   \n",
      "27  (pitt, angelina, jolie)            (brad)            0.004224   \n",
      "26      (pitt, brad, jolie)        (angelina)            0.004224   \n",
      "11                 (markle)          (meghan)            0.010837   \n",
      "7                   (gomez)          (selena)            0.010281   \n",
      "25         (prince, markle)          (meghan)            0.005947   \n",
      "3                 (shelton)           (blake)            0.005363   \n",
      "29            (pitt, jolie)  (brad, angelina)            0.004252   \n",
      "5                  (carpet)             (red)            0.005863   \n",
      "19         (bieber, selena)          (justin)            0.004307   \n",
      "2                  (bieber)          (justin)            0.007586   \n",
      "23          (harry, markle)          (prince)            0.005419   \n",
      "\n",
      "    consequent support   support  confidence        lift  leverage   \n",
      "21            0.012532  0.004807    1.000000   79.793792  0.004747  \\\n",
      "12            0.044544  0.004585    1.000000   22.449782  0.004381   \n",
      "4             0.013227  0.010976    0.994962   75.222070  0.010830   \n",
      "22            0.016173  0.005391    0.994872   61.516239  0.005303   \n",
      "31            0.016173  0.005252    0.994737   61.507895  0.005167   \n",
      "16            0.013227  0.005196    0.994681   75.200798  0.005127   \n",
      "8             0.005419  0.004752    0.994186  183.475760  0.004726   \n",
      "20            0.013227  0.004224    0.993464   75.108804  0.004168   \n",
      "18            0.009670  0.004224    0.993464  102.735031  0.004183   \n",
      "27            0.013227  0.004196    0.993421   75.105553  0.004140   \n",
      "26            0.009670  0.004196    0.993421  102.730585  0.004155   \n",
      "11            0.016173  0.010754    0.992308   61.357692  0.010579   \n",
      "7             0.012532  0.010198    0.991892   79.146815  0.010069   \n",
      "25            0.016173  0.005891    0.990654   61.255452  0.005795   \n",
      "3             0.008559  0.005307    0.989637  115.630122  0.005262   \n",
      "29            0.005724  0.004196    0.986928  172.410591  0.004172   \n",
      "5             0.007419  0.005780    0.985782  132.866429  0.005736   \n",
      "19            0.016284  0.004224    0.980645   60.222658  0.004154   \n",
      "2             0.016284  0.007419    0.978022   60.061565  0.007296   \n",
      "23            0.014394  0.005280    0.974359   67.691615  0.005202   \n",
      "\n",
      "    conviction  zhangs_metric  \n",
      "21         inf       0.992238  \n",
      "12         inf       0.959857  \n",
      "4   195.874441       0.997713  \n",
      "22  191.846361       0.989104  \n",
      "31  186.927224       0.988963  \n",
      "16  185.513324       0.991884  \n",
      "8   171.067997       0.999326  \n",
      "20  150.976269       0.990899  \n",
      "18  151.520466       0.994494  \n",
      "27  149.989496       0.990871  \n",
      "26  150.530136       0.994466  \n",
      "11  127.897574       0.994480  \n",
      "7   121.787683       0.997622  \n",
      "25  105.269542       0.989559  \n",
      "3    95.674091       0.996697  \n",
      "29   76.062092       0.998445  \n",
      "5    69.811506       0.998327  \n",
      "19   50.825344       0.987649  \n",
      "2    44.759094       0.990867  \n",
      "23   38.438631       0.990595  \n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                antecedents       consequents  antecedent support   \n",
      "0                   (jolie)        (angelina)            0.008170  \\\n",
      "1                 (aniston)        (jennifer)            0.008142   \n",
      "2                  (bieber)          (justin)            0.007586   \n",
      "3                 (shelton)           (blake)            0.005363   \n",
      "4                    (pitt)            (brad)            0.011032   \n",
      "5                  (carpet)             (red)            0.005863   \n",
      "6                  (disick)           (scott)            0.004252   \n",
      "7                   (gomez)          (selena)            0.010281   \n",
      "8                 (stefani)            (gwen)            0.004780   \n",
      "9                   (lopez)        (jennifer)            0.004863   \n",
      "10                   (katy)           (perry)            0.004557   \n",
      "11                 (markle)          (meghan)            0.010837   \n",
      "12                   (york)             (new)            0.004585   \n",
      "13                 (united)          (states)            0.011087   \n",
      "14                  (swift)          (taylor)            0.007781   \n",
      "15            (brad, jolie)        (angelina)            0.004863   \n",
      "16         (pitt, angelina)            (brad)            0.005224   \n",
      "17         (brad, angelina)            (pitt)            0.005724   \n",
      "18            (pitt, jolie)        (angelina)            0.004252   \n",
      "19         (bieber, selena)          (justin)            0.004307   \n",
      "20            (pitt, jolie)            (brad)            0.004252   \n",
      "21          (justin, gomez)          (selena)            0.004807   \n",
      "22          (harry, markle)          (meghan)            0.005419   \n",
      "23          (harry, markle)          (prince)            0.005419   \n",
      "24       (west, kardashian)             (kim)            0.004168   \n",
      "25         (prince, markle)          (meghan)            0.005947   \n",
      "26      (pitt, brad, jolie)        (angelina)            0.004224   \n",
      "27  (pitt, angelina, jolie)            (brad)            0.004224   \n",
      "28  (brad, angelina, jolie)            (pitt)            0.004641   \n",
      "29            (pitt, jolie)  (brad, angelina)            0.004252   \n",
      "30  (harry, meghan, markle)          (prince)            0.005391   \n",
      "31  (harry, prince, markle)          (meghan)            0.005280   \n",
      "32          (harry, markle)  (meghan, prince)            0.005419   \n",
      "\n",
      "    consequent support   support  confidence        lift  leverage   \n",
      "0             0.009670  0.007892    0.965986   99.893541  0.007813  \\\n",
      "1             0.022230  0.007864    0.965870   43.448468  0.007683   \n",
      "2             0.016284  0.007419    0.978022   60.061565  0.007296   \n",
      "3             0.008559  0.005307    0.989637  115.630122  0.005262   \n",
      "4             0.013227  0.010976    0.994962   75.222070  0.010830   \n",
      "5             0.007419  0.005780    0.985782  132.866429  0.005736   \n",
      "6             0.015478  0.004085    0.960784   62.074946  0.004019   \n",
      "7             0.012532  0.010198    0.991892   79.146815  0.010069   \n",
      "8             0.005419  0.004752    0.994186  183.475760  0.004726   \n",
      "9             0.022230  0.004641    0.954286   42.927350  0.004532   \n",
      "10            0.006558  0.004113    0.902439  137.610480  0.004083   \n",
      "11            0.016173  0.010754    0.992308   61.357692  0.010579   \n",
      "12            0.044544  0.004585    1.000000   22.449782  0.004381   \n",
      "13            0.018646  0.010365    0.934837   50.137083  0.010158   \n",
      "14            0.011699  0.007558    0.971429   83.037530  0.007467   \n",
      "15            0.009670  0.004641    0.954286   98.683563  0.004594   \n",
      "16            0.013227  0.005196    0.994681   75.200798  0.005127   \n",
      "17            0.011032  0.005196    0.907767   82.286677  0.005133   \n",
      "18            0.009670  0.004224    0.993464  102.735031  0.004183   \n",
      "19            0.016284  0.004224    0.980645   60.222658  0.004154   \n",
      "20            0.013227  0.004224    0.993464   75.108804  0.004168   \n",
      "21            0.012532  0.004807    1.000000   79.793792  0.004747   \n",
      "22            0.016173  0.005391    0.994872   61.516239  0.005303   \n",
      "23            0.014394  0.005280    0.974359   67.691615  0.005202   \n",
      "24            0.019729  0.004057    0.973333   49.334291  0.003975   \n",
      "25            0.016173  0.005891    0.990654   61.255452  0.005795   \n",
      "26            0.009670  0.004196    0.993421  102.730585  0.004155   \n",
      "27            0.013227  0.004196    0.993421   75.105553  0.004140   \n",
      "28            0.011032  0.004196    0.904192   81.962579  0.004145   \n",
      "29            0.005724  0.004196    0.986928  172.410591  0.004172   \n",
      "30            0.014394  0.005252    0.974227   67.682432  0.005174   \n",
      "31            0.016173  0.005252    0.994737   61.507895  0.005167   \n",
      "32            0.007030  0.005252    0.969231  137.864457  0.005214   \n",
      "\n",
      "    conviction  zhangs_metric  \n",
      "0    29.115697       0.998144  \n",
      "1    28.648654       0.985004  \n",
      "2    44.759094       0.990867  \n",
      "3    95.674091       0.996697  \n",
      "4   195.874441       0.997713  \n",
      "5    69.811506       0.998327  \n",
      "6    25.105316       0.988091  \n",
      "7   121.787683       0.997622  \n",
      "8   171.067997       0.999326  \n",
      "9    21.388713       0.981478  \n",
      "10   10.182781       0.997278  \n",
      "11  127.897574       0.994480  \n",
      "12         inf       0.959857  \n",
      "13   15.060015       0.991043  \n",
      "14   34.590547       0.995704  \n",
      "15   21.663465       0.994704  \n",
      "16  185.513324       0.991884  \n",
      "17   10.722498       0.993535  \n",
      "18  151.520466       0.994494  \n",
      "19   50.825344       0.987649  \n",
      "20  150.976269       0.990899  \n",
      "21         inf       0.992238  \n",
      "22  191.846361       0.989104  \n",
      "23   38.438631       0.990595  \n",
      "24   36.760149       0.983831  \n",
      "25  105.269542       0.989559  \n",
      "26  150.530136       0.994466  \n",
      "27  149.989496       0.990871  \n",
      "28   10.322356       0.992405  \n",
      "29   76.062092       0.998445  \n",
      "30   38.241509       0.990565  \n",
      "31  186.927224       0.988963  \n",
      "32   32.271515       0.998155  \n"
     ]
    }
   ],
   "source": [
    "# Set max_rows to display all the results\n",
    "pd.options.display.max_rows = len(rules_All)\n",
    "\n",
    "# Print the association rules dataframe\n",
    "print(rules_All)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Fig. 4 and Table 4\n",
    "# Example dataset of news articles with keywords\n",
    "\n",
    "onlyreal_l = dataset[dataset['label'] == 1]\n",
    "onlyreal_s = onlyreal_l[['statement']]\n",
    "\n",
    "# Step 1: Load sentences\n",
    "sentences = onlyreal_s['statement'].tolist()\n",
    "# Preprocessing steps here, e.g., tokenization, stopword removal, etc.\n",
    "\n",
    "# Step 2: Convert sentences to transactions\n",
    "transactions = []\n",
    "for sentence in sentences:\n",
    "    transaction = str(sentence).split()  # Split sentence into items (words)\n",
    "    transactions.append(transaction)\n",
    "\n",
    "# Step 3: Create transaction dataset\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit_transform(transactions)\n",
    "transaction_df = pd.DataFrame(te_ary, columns=te.columns_)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gglMhfAYpthV",
    "outputId": "b45e70d1-cef5-4e2b-a387-f33f4d031ab8",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(transaction_df.shape)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ne8Uz3_ijh4e",
    "outputId": "7a881266-8040-4691-a2e6-8771c9fecd17"
   },
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24575, 23390)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "frequent_itemsets = apriori(transaction_df, min_support=0.004, use_colnames=True)\n",
    "# 0.004 ~ occur at least 100 (exacly 98.3) times out of a total of 24575 transactions"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AUG11TjXvA0x",
    "outputId": "51fcfcc2-af4c-484a-d2ea-480f91364f8d",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(frequent_itemsets.shape)"
   ],
   "metadata": {
    "id": "c40AqOMEo0kC"
   },
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(421, 2)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.9)\n",
    "selected_columns = ['antecedents', 'consequents', 'support', 'confidence']\n",
    "print(rules.loc[:, selected_columns])"
   ],
   "metadata": {
    "id": "fjII7QNJzGRN"
   },
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                antecedents       consequents   support  confidence\n",
      "0                  (carpet)             (red)  0.008057    0.994975\n",
      "1                   (gomez)          (selena)  0.006307    0.993590\n",
      "2              (housewives)            (real)  0.004924    0.952756\n",
      "3                  (markle)          (meghan)  0.009237    0.991266\n",
      "4                    (york)             (new)  0.005005    1.000000\n",
      "5                  (united)          (states)  0.009766    0.952381\n",
      "6                   (swift)          (taylor)  0.006633    0.964497\n",
      "7           (harry, markle)          (meghan)  0.004720    1.000000\n",
      "8           (harry, markle)          (prince)  0.004639    0.982759\n",
      "9           (harry, meghan)          (prince)  0.005738    0.940000\n",
      "10         (prince, markle)          (meghan)  0.005249    0.992308\n",
      "11  (harry, meghan, markle)          (prince)  0.004639    0.982759\n",
      "12  (harry, prince, markle)          (meghan)  0.004639    1.000000\n",
      "13          (harry, markle)  (meghan, prince)  0.004639    0.982759\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Set max_rows to display all the results\n",
    "pd.options.display.max_rows = len(rules)\n",
    "\n",
    "# Print the association rules dataframe\n",
    "print(rules)"
   ],
   "metadata": {
    "id": "9hH1D21SzvTP"
   },
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                antecedents       consequents  antecedent support   \n",
      "0                  (carpet)             (red)            0.008098  \\\n",
      "1                   (gomez)          (selena)            0.006348   \n",
      "2              (housewives)            (real)            0.005168   \n",
      "3                  (markle)          (meghan)            0.009318   \n",
      "4                    (york)             (new)            0.005005   \n",
      "5                  (united)          (states)            0.010254   \n",
      "6                   (swift)          (taylor)            0.006877   \n",
      "7           (harry, markle)          (meghan)            0.004720   \n",
      "8           (harry, markle)          (prince)            0.004720   \n",
      "9           (harry, meghan)          (prince)            0.006104   \n",
      "10         (prince, markle)          (meghan)            0.005290   \n",
      "11  (harry, meghan, markle)          (prince)            0.004720   \n",
      "12  (harry, prince, markle)          (meghan)            0.004639   \n",
      "13          (harry, markle)  (meghan, prince)            0.004720   \n",
      "\n",
      "    consequent support   support  confidence        lift  leverage   \n",
      "0             0.009847  0.008057    0.994975  101.039287  0.007977  \\\n",
      "1             0.008627  0.006307    0.993590  115.176736  0.006252   \n",
      "2             0.009440  0.004924    0.952756  100.922312  0.004875   \n",
      "3             0.014568  0.009237    0.991266   68.045730  0.009101   \n",
      "4             0.046144  0.005005    1.000000   21.671076  0.004774   \n",
      "5             0.017416  0.009766    0.952381   54.684023  0.009587   \n",
      "6             0.011679  0.006633    0.964497   82.587160  0.006552   \n",
      "7             0.014568  0.004720    1.000000   68.645251  0.004651   \n",
      "8             0.014690  0.004639    0.982759   66.901089  0.004570   \n",
      "9             0.014690  0.005738    0.940000   63.990305  0.005648   \n",
      "10            0.014568  0.005249    0.992308   68.117211  0.005172   \n",
      "11            0.014690  0.004639    0.982759   66.901089  0.004570   \n",
      "12            0.014568  0.004639    1.000000   68.645251  0.004571   \n",
      "13            0.006389  0.004639    0.982759  153.829892  0.004609   \n",
      "\n",
      "    conviction  zhangs_metric  \n",
      "0   197.040366       0.998186  \n",
      "1   154.654242       0.997651  \n",
      "2    20.966843       0.995235  \n",
      "3   112.832004       0.994572  \n",
      "4          inf       0.958654  \n",
      "5    20.634262       0.991884  \n",
      "6    27.837721       0.994732  \n",
      "7          inf       0.990106  \n",
      "8    57.147996       0.989724  \n",
      "9    16.421838       0.990418  \n",
      "10  128.106205       0.990559  \n",
      "11   57.147996       0.989724  \n",
      "12         inf       0.990025  \n",
      "13   57.629461       0.998211  \n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "antecedents_set = set(rules['antecedents'].tolist())\n",
    "consequents_set = set(rules['consequents'].tolist())\n",
    "combined_set = antecedents_set | consequents_set\n",
    "\n",
    "words = [list(word_set)[0] for word_set in combined_set]\n",
    "unique_words = set(words)\n",
    "print(unique_words)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E1J6hJEI0Kz1",
    "outputId": "41916bb1-7d25-493b-de1a-6eb1d6c20850"
   },
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'york', 'meghan', 'prince', 'housewives', 'new', 'states', 'united', 'selena', 'taylor', 'real', 'gomez', 'red', 'carpet', 'harry', 'swift', 'markle'}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Sort the rules based on support in descending order\n",
    "sorted_rules = rules.sort_values('confidence', ascending=False)\n",
    "\n",
    "# Print the top 10 rules with highest support\n",
    "print(sorted_rules.head(20))\n"
   ],
   "metadata": {
    "id": "Ero6M8O6z2Lt"
   },
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                antecedents       consequents  antecedent support   \n",
      "4                    (york)             (new)            0.005005  \\\n",
      "7           (harry, markle)          (meghan)            0.004720   \n",
      "12  (harry, prince, markle)          (meghan)            0.004639   \n",
      "0                  (carpet)             (red)            0.008098   \n",
      "1                   (gomez)          (selena)            0.006348   \n",
      "10         (prince, markle)          (meghan)            0.005290   \n",
      "3                  (markle)          (meghan)            0.009318   \n",
      "8           (harry, markle)          (prince)            0.004720   \n",
      "11  (harry, meghan, markle)          (prince)            0.004720   \n",
      "13          (harry, markle)  (meghan, prince)            0.004720   \n",
      "6                   (swift)          (taylor)            0.006877   \n",
      "2              (housewives)            (real)            0.005168   \n",
      "5                  (united)          (states)            0.010254   \n",
      "9           (harry, meghan)          (prince)            0.006104   \n",
      "\n",
      "    consequent support   support  confidence        lift  leverage   \n",
      "4             0.046144  0.005005    1.000000   21.671076  0.004774  \\\n",
      "7             0.014568  0.004720    1.000000   68.645251  0.004651   \n",
      "12            0.014568  0.004639    1.000000   68.645251  0.004571   \n",
      "0             0.009847  0.008057    0.994975  101.039287  0.007977   \n",
      "1             0.008627  0.006307    0.993590  115.176736  0.006252   \n",
      "10            0.014568  0.005249    0.992308   68.117211  0.005172   \n",
      "3             0.014568  0.009237    0.991266   68.045730  0.009101   \n",
      "8             0.014690  0.004639    0.982759   66.901089  0.004570   \n",
      "11            0.014690  0.004639    0.982759   66.901089  0.004570   \n",
      "13            0.006389  0.004639    0.982759  153.829892  0.004609   \n",
      "6             0.011679  0.006633    0.964497   82.587160  0.006552   \n",
      "2             0.009440  0.004924    0.952756  100.922312  0.004875   \n",
      "5             0.017416  0.009766    0.952381   54.684023  0.009587   \n",
      "9             0.014690  0.005738    0.940000   63.990305  0.005648   \n",
      "\n",
      "    conviction  zhangs_metric  \n",
      "4          inf       0.958654  \n",
      "7          inf       0.990106  \n",
      "12         inf       0.990025  \n",
      "0   197.040366       0.998186  \n",
      "1   154.654242       0.997651  \n",
      "10  128.106205       0.990559  \n",
      "3   112.832004       0.994572  \n",
      "8    57.147996       0.989724  \n",
      "11   57.147996       0.989724  \n",
      "13   57.629461       0.998211  \n",
      "6    27.837721       0.994732  \n",
      "2    20.966843       0.995235  \n",
      "5    20.634262       0.991884  \n",
      "9    16.421838       0.990418  \n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Fig. 5\n",
    "\n",
    "onlyfake_l = dataset[dataset['label'] == 0]\n",
    "onlyfake_n = onlyfake_l[['statement']]\n",
    "\n",
    "# Step 1: Load sentences\n",
    "sentences_n = onlyfake_n['statement'].tolist()\n",
    "# Preprocessing steps here, e.g., tokenization, stopword removal, etc.\n",
    "\n",
    "# Step 2: Convert sentences to transactions\n",
    "transactions_n = []\n",
    "for sentence_n in sentences_n:\n",
    "    transaction_n = str(sentence_n).split()  # Split sentence into items (words)\n",
    "    transactions_n.append(transaction_n)\n",
    "\n",
    "# Step 3: Create transaction dataset\n",
    "te_n = TransactionEncoder()\n",
    "te_ary_n = te_n.fit_transform(transactions_n)\n",
    "transaction_df_n = pd.DataFrame(te_ary_n, columns=te_n.columns_)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qqbUcAvP2QJ-",
    "outputId": "d2376d59-ce45-4464-d7bd-702d688a8724",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "frequent_itemsets_n = apriori(transaction_df_n, min_support=0.002, use_colnames=True)"
   ],
   "metadata": {
    "id": "4nVyGapBwdex",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 22,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          antecedents                 consequents   support   \n",
      "0                           (affleck)                       (ben)  0.007624  \\\n",
      "1                         (rodriguez)                      (alex)  0.004907   \n",
      "2                             (jolie)                  (angelina)  0.020592   \n",
      "3                            (jolies)                  (angelina)  0.003593   \n",
      "4                           (aniston)                  (jennifer)  0.020680   \n",
      "..                                ...                         ...       ...   \n",
      "480  (jennifer, brad, jolie, aniston)            (pitt, angelina)  0.002103   \n",
      "481      (pitt, brad, jolie, aniston)        (jennifer, angelina)  0.002103   \n",
      "482           (jennifer, jolie, pitt)   (brad, angelina, aniston)  0.002103   \n",
      "483            (pitt, jolie, aniston)  (jennifer, brad, angelina)  0.002103   \n",
      "484            (brad, jolie, aniston)  (jennifer, angelina, pitt)  0.002103   \n",
      "\n",
      "     confidence  \n",
      "0      0.966667  \n",
      "1      0.982456  \n",
      "2      0.963115  \n",
      "3      1.000000  \n",
      "4      0.959350  \n",
      "..          ...  \n",
      "480    0.960000  \n",
      "481    1.000000  \n",
      "482    0.923077  \n",
      "483    1.000000  \n",
      "484    0.923077  \n",
      "\n",
      "[485 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "rules_n = association_rules(frequent_itemsets_n, metric=\"confidence\", min_threshold=0.9)\n",
    "selected_columns = ['antecedents', 'consequents', 'support', 'confidence']\n",
    "rules_n.to_csv('fake_news_rules.csv', index=False)\n",
    "print(rules_n.loc[:, selected_columns])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lady', 'kendall', 'robbie', 'holmes', 'jenner', 'travis', 'blake', 'kanye', 'obamas', 'care', 'markles', 'plan', 'gomez', 'health', 'swift', 'government', 'justin', 'lawrence', 'pratt', 'garner', 'wedding', 'miranda', 'ellen', 'markle', 'richie', 'royal', 'kids', 'weeknd', 'chris', 'walker', 'court', 'planned', 'bendjima', 'kourtney', 'styles', 'neri', 'united', 'foxx', 'degeneres', 'khloe', 'biebers', 'spears', 'divorce', 'miley', 'rubio', 'split', 'affleck', 'selena', 'stewart', 'hemsworth', 'pattinson', 'kidman', 'jamie', 'kim', 'sofia', 'jersey', 'bieber', 'parenthood', 'jolies', 'mariah', 'supreme', 'pitt', 'perry', 'de', 'rossi', 'gaga', 'aniston', 'cyrus', 'affordable', 'portia', 'kristen', 'meghan', 'law', 'wests', 'york', 'kylie', 'younes', 'jennifer', 'cruise', 'brad', 'kardashian', 'keith', 'prince', 'states', 'jolie', 'taylor', 'kate', 'marco', 'lambert', 'west', 'act', 'alex', 'britney', 'angelina', 'custody', 'thompson', 'scott', 'pitts', 'katy', 'harry', 'theroux', 'carey', 'kris', 'katie', 'gov', 'jameel', 'rodriguez', 'stefani', 'tristan', 'gwen', 'nicole', 'urban', 'robert', 'new', 'shelton', 'middleton', 'margot', 'liam', 'ben', 'disick', 'hassan', 'lopez'}\n"
     ]
    }
   ],
   "source": [
    "antecedents_set_n = set(rules_n['antecedents'].tolist())\n",
    "consequents_set_n = set(rules_n['consequents'].tolist())\n",
    "combined_set_n = antecedents_set_n | consequents_set_n\n",
    "\n",
    "words_n = [list(word_set_n)[0] for word_set_n in combined_set_n]\n",
    "unique_words_n = set(words_n)\n",
    "print(unique_words_n)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 antecedents   consequents  antecedent support   \n",
      "378    (holmes, jamie, foxx)       (katie)            0.006835  \\\n",
      "339      (portia, degeneres)   (ellen, de)            0.002278   \n",
      "166                  (rossi)  (portia, de)            0.002541   \n",
      "167      (portia, degeneres)       (ellen)            0.002278   \n",
      "168       (degeneres, rossi)       (ellen)            0.002191   \n",
      "..                       ...           ...                 ...   \n",
      "338   (ellen, degeneres, de)      (portia)            0.002278   \n",
      "203         (harry, wedding)      (prince)            0.002454   \n",
      "181           (ellen, rossi)      (portia)            0.002454   \n",
      "337  (portia, de, degeneres)       (ellen)            0.002278   \n",
      "183           (holmes, foxx)       (jamie)            0.006835   \n",
      "\n",
      "     consequent support   support  confidence        lift  leverage   \n",
      "378            0.011129  0.006835         1.0   89.858268  0.006759  \\\n",
      "339            0.002541  0.002278         1.0  393.517241  0.002273   \n",
      "166            0.002629  0.002541         1.0  380.400000  0.002535   \n",
      "167            0.003943  0.002278         1.0  253.600000  0.002269   \n",
      "168            0.003943  0.002191         1.0  253.600000  0.002182   \n",
      "..                  ...       ...         ...         ...       ...   \n",
      "338            0.002804  0.002278         1.0  356.625000  0.002272   \n",
      "203            0.013757  0.002454         1.0   72.687898  0.002420   \n",
      "181            0.002804  0.002454         1.0  356.625000  0.002447   \n",
      "337            0.003943  0.002278         1.0  253.600000  0.002269   \n",
      "183            0.009026  0.006835         1.0  110.796117  0.006773   \n",
      "\n",
      "     conviction  zhangs_metric  \n",
      "378         inf       0.995677  \n",
      "339         inf       0.999737  \n",
      "166         inf       0.999912  \n",
      "167         inf       0.998331  \n",
      "168         inf       0.998244  \n",
      "..          ...            ...  \n",
      "338         inf       0.999473  \n",
      "203         inf       0.988668  \n",
      "181         inf       0.999649  \n",
      "337         inf       0.998331  \n",
      "183         inf       0.997794  \n",
      "\n",
      "[20 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "# Sort the rules based on support in descending order\n",
    "sorted_rules = rules_n.sort_values('confidence', ascending=False)\n",
    "\n",
    "# Print the top 10 rules with highest support\n",
    "print(sorted_rules.head(20))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}