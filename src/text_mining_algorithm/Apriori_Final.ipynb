{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nLzjvZmTg0N4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load Liar dataset\n",
        "liar_cols = [\"id\", \"label\", \"statement\"]\n",
        "liar_path_test = \"test.tsv\"\n",
        "liar_df_test = pd.read_csv(liar_path_test, sep='\\t', header=None, usecols=[0, 1, 2], names=liar_cols)\n",
        "liar_path_train = \"train.tsv\"\n",
        "liar_df_train = pd.read_csv(liar_path_train, sep='\\t', header=None, usecols=[0, 1, 2], names=liar_cols)\n",
        "liar_path_valid = \"valid.tsv\"\n",
        "liar_df_valid = pd.read_csv(liar_path_valid, sep='\\t', header=None, usecols=[0, 1, 2], names=liar_cols)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load FakeNewsNet dataset\n",
        "#fnn_cols = [\"id\",\"news_url\", \"title\", \"tweet_id\"]\n",
        "fnn_cols = [\"id\",\"statement\",]\n",
        "\n",
        "fnn_path_gossip_fake = \"gossipcop_fake.csv\"\n",
        "fnn_df_gossip_fake = pd.read_csv(fnn_path_gossip_fake, usecols=[0, 2], names=fnn_cols, header = 0)\n",
        "fnn_df_gossip_fake[\"label\"] = \"false\"\n",
        "\n",
        "fnn_path_gossip_real = \"gossipcop_real.csv\"\n",
        "fnn_df_gossip_real = pd.read_csv(fnn_path_gossip_real, usecols=[0,2], names=fnn_cols,  header = 0)\n",
        "fnn_df_gossip_real[\"label\"] = \"true\"\n",
        "\n",
        "fnn_path_polit_fake = \"politifact_fake.csv\"\n",
        "fnn_df_polit_fake = pd.read_csv(fnn_path_polit_fake, usecols=[0, 2], names=fnn_cols,  header = 0)\n",
        "fnn_df_polit_fake[\"label\"] = \"false\"\n",
        "\n",
        "fnn_path_polit_real = \"politifact_real.csv\"\n",
        "fnn_df_polit_real = pd.read_csv(fnn_path_polit_real, usecols=[0, 2], names=fnn_cols,  header = 0)\n",
        "fnn_df_polit_real[\"label\"] = \"true\"\n",
        "combined_liar = pd.concat([liar_df_test, liar_df_train, liar_df_valid])\n",
        "# Removing rows with missing values\n",
        "combined_liar.dropna(inplace=True)\n",
        "# Converting labels to numerical values\n",
        "#I hate this! We lose so much information!\n",
        "combined_liar['label'] = combined_liar['label'].map({'pants-fire': 0, 'false': 0, 'barely-true': 0, 'half-true': 1, 'mostly-true': 1, 'true': 1})\n"
      ],
      "metadata": {
        "id": "j-U-2J9Ch2LZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_fnn  = pd.concat([fnn_df_gossip_fake, fnn_df_gossip_real, fnn_df_polit_fake, fnn_df_polit_real])\n",
        "# Removing rows with missing values\n",
        "combined_fnn.dropna(inplace=True)\n",
        "# Converting labels to numerical values\n",
        "combined_fnn['label'] = combined_fnn['label'].map({'false': 0, 'true': 1})"
      ],
      "metadata": {
        "id": "yB2WwbK7iFeI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_liar_fnn =pd.concat([combined_liar[[\"statement\",\"label\"]], combined_fnn])"
      ],
      "metadata": {
        "id": "n3ysKfjgiJTy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Define a function to preprocess text data\n",
        "def preprocess_text(text):\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation+'-–‘’“”' ))\n",
        "    \n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    \n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "    text = ' '.join(filtered_tokens)\n",
        "    \n",
        "    return text\n",
        "\n",
        "# Preprocess the text data\n",
        "combined_liar_fnn['statement'] = combined_liar_fnn['statement'].apply(preprocess_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1vAkeYtiO3y",
        "outputId": "b51cdfd7-0e5d-4496-c4fb-081867eef895"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import mlxtend\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "from mlxtend.frequent_patterns import apriori, association_rules\n",
        "import pandas as pd\n",
        "\n",
        "# Example dataset of news articles with keywords\n",
        "dataset = combined_liar_fnn\n",
        "dataset = dataset[['statement','label']]"
      ],
      "metadata": {
        "id": "2BxF9rJKxldC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%%%%%%%%%%% All data %%%%%%%%%%%\n",
        "# Figs. 2 and 3\n",
        "All = dataset\n",
        "\n",
        "All_s = All[['statement']]\n",
        "\n",
        "# Step 1: Load sentences\n",
        "sentences_All = All_s['statement'].tolist()\n",
        "# Preprocessing steps here, e.g., tokenization, stopword removal, etc.\n",
        "\n",
        "# Step 2: Convert sentences to transactions\n",
        "transactions_All = []\n",
        "for sentence in sentences_All:\n",
        "    transaction_All = sentence.split()  # Split sentence into items (words)\n",
        "    transactions_All.append(transaction_All)\n",
        "\n",
        "# Step 3: Create transaction dataset\n",
        "te_All = TransactionEncoder()\n",
        "te_ary_All = te_All.fit_transform(transactions_All)\n",
        "transaction_df_All = pd.DataFrame(te_ary_All, columns=te_All.columns_)"
      ],
      "metadata": {
        "id": "cWVEGYeyo6gk"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Support guide calculation\n",
        "sentences = transactions_All\n",
        "word1 = \"kardashian\"\n",
        "word2 = \"kim\"\n",
        "\n",
        "count = 0\n",
        "for sentence in sentences:\n",
        "    if word1 in sentence and word2 in sentence:\n",
        "        count += 1\n",
        "\n",
        "print(f\"The words '{word1}' and '{word2}' appear together in {count} sentences.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W01wdPQQ3_qC",
        "outputId": "aff17cee-3623-4ddd-f404-c8127af7994a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The words 'kardashian' and 'kim' appear together in 419 sentences.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(transaction_df_All.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gP2W32lTLjy",
        "outputId": "4214dc26-d726-4b17-e22c-fee28ad08ef8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(30554, 25958)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#%% Step 4: Apply Apriori algorithm\n",
        "frequent_itemsets_All = apriori(transaction_df_All, min_support=0.004, use_colnames=True)"
      ],
      "metadata": {
        "id": "zIZbnuW1wbQ0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(frequent_itemsets_All.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkBTsbtId14K",
        "outputId": "c765dbb7-a298-48cd-89fb-c806300c3cf3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(501, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rules_All = association_rules(frequent_itemsets_All, metric=\"confidence\", min_threshold=0.9)"
      ],
      "metadata": {
        "id": "Ai4pKJYawc3c"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_columns = ['antecedents', 'consequents', 'support', 'confidence']\n",
        "print(rules_All.loc[:, selected_columns])"
      ],
      "metadata": {
        "id": "VLUmLazKlxIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "antecedents_set_All = set(rules_All['antecedents'].tolist())\n",
        "consequents_set_All = set(rules_All['consequents'].tolist())\n",
        "combined_set_All = antecedents_set_All | consequents_set_All\n",
        "\n",
        "words_All = [list(word_set_All)[0] for word_set_All in combined_set_All]\n",
        "unique_words_All = set(words_All)\n",
        "print(unique_words_All)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvbmLYmSwhMb",
        "outputId": "195c8a2c-c891-47f4-aec5-ca3bdd2aac1a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'carpet', 'new', 'pitt', 'theroux', 'gomez', 'taylor', 'gwen', 'stefani', 'states', 'jolie', 'brad', 'katie', 'swift', 'united', 'justin', 'aniston', 'blake', 'bieber', 'markle', 'york', 'jennifer', 'selena', 'prince', 'meghan', 'holmes', 'red', 'shelton', 'lopez', 'angelina'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort the rules based on support in descending order\n",
        "sorted_rules = rules_All.sort_values('confidence', ascending=False)\n",
        "\n",
        "# Print the top 10 rules with highest support\n",
        "print(sorted_rules.head(20))"
      ],
      "metadata": {
        "id": "Rj2ZTW8hyEpA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0545a96-a7d9-4866-fd2b-22609668fda1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                antecedents       consequents  antecedent support  \\\n",
            "12                   (york)             (new)            0.004615   \n",
            "10                (theroux)          (justin)            0.004058   \n",
            "22         (stefani, blake)            (gwen)            0.004353   \n",
            "20          (gwen, shelton)           (blake)            0.004026   \n",
            "24          (gomez, justin)          (selena)            0.005237   \n",
            "4                    (pitt)            (brad)            0.012568   \n",
            "16         (pitt, angelina)            (brad)            0.005924   \n",
            "3                 (shelton)           (blake)            0.005728   \n",
            "27         (markle, prince)          (meghan)            0.005531   \n",
            "6                   (gomez)          (selena)            0.010506   \n",
            "7                 (stefani)            (gwen)            0.005204   \n",
            "11                 (markle)          (meghan)            0.010146   \n",
            "25          (markle, harry)          (meghan)            0.005073   \n",
            "32  (markle, harry, prince)          (meghan)            0.004942   \n",
            "23            (pitt, jolie)            (brad)            0.004844   \n",
            "18            (pitt, jolie)        (angelina)            0.004844   \n",
            "28      (pitt, brad, jolie)        (angelina)            0.004811   \n",
            "29  (pitt, jolie, angelina)            (brad)            0.004811   \n",
            "8                  (holmes)           (katie)            0.004517   \n",
            "31            (pitt, jolie)  (brad, angelina)            0.004844   \n",
            "\n",
            "    consequent support   support  confidence        lift  leverage  conviction  \n",
            "12            0.043824  0.004615    1.000000   22.818521  0.004413         inf  \n",
            "10            0.017085  0.004058    1.000000   58.532567  0.003989         inf  \n",
            "22            0.005924  0.004353    1.000000  168.806630  0.004327         inf  \n",
            "20            0.008608  0.004026    1.000000  116.174905  0.003991         inf  \n",
            "24            0.012273  0.005237    1.000000   81.477333  0.005172         inf  \n",
            "4             0.014924  0.012502    0.994792   66.655405  0.012315  189.134516  \n",
            "16            0.014924  0.005891    0.994475   66.634196  0.005803  178.298684  \n",
            "3             0.008608  0.005695    0.994286  115.511048  0.005646  173.493651  \n",
            "27            0.015055  0.005498    0.994083   66.028711  0.005415  166.455652  \n",
            "6             0.012273  0.010441    0.993769   80.969686  0.010312  158.530127  \n",
            "7             0.005924  0.005171    0.993711  167.744953  0.005140  158.058094  \n",
            "11            0.015055  0.010081    0.993548   65.993212  0.009928  152.666427  \n",
            "25            0.015055  0.005040    0.993548   65.993212  0.004964  152.666427  \n",
            "32            0.015055  0.004909    0.993377   65.981860  0.004835  148.726648  \n",
            "23            0.014924  0.004811    0.993243   66.551654  0.004739  145.791189  \n",
            "18            0.010735  0.004811    0.993243   92.523031  0.004759  146.411206  \n",
            "28            0.010735  0.004778    0.993197   92.518749  0.004727  145.421941  \n",
            "29            0.014924  0.004778    0.993197   66.548574  0.004707  144.806114  \n",
            "8             0.005466  0.004484    0.992754  181.632301  0.004459  137.245729  \n",
            "31            0.006480  0.004778    0.986486  152.227819  0.004747   73.520456  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set max_rows to display all the results\n",
        "pd.options.display.max_rows = len(rules_All)\n",
        "\n",
        "# Print the association rules dataframe\n",
        "print(rules_All)"
      ],
      "metadata": {
        "id": "dApD96OfiTdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%%%%%%%%%%% Get only real news %%%%%%%%%%%\n",
        "# Fig. 4 and Table 4\n",
        "# Example dataset of news articles with keywords\n",
        "dataset = combined_liar_fnn\n",
        "dataset = dataset[['statement','label']]\n",
        "\n",
        "onlyreal_l = dataset[dataset['label'] == 1]\n",
        "onlyreal_s = onlyreal_l[['statement']]\n",
        "\n",
        "# Step 1: Load sentences\n",
        "sentences = onlyreal_s['statement'].tolist()\n",
        "# Preprocessing steps here, e.g., tokenization, stopword removal, etc.\n",
        "\n",
        "# Step 2: Convert sentences to transactions\n",
        "transactions = []\n",
        "for sentence in sentences:\n",
        "    transaction = sentence.split()  # Split sentence into items (words)\n",
        "    transactions.append(transaction)\n",
        "\n",
        "# Step 3: Create transaction dataset\n",
        "te = TransactionEncoder()\n",
        "te_ary = te.fit_transform(transactions)\n",
        "transaction_df = pd.DataFrame(te_ary, columns=te.columns_)"
      ],
      "metadata": {
        "id": "eLyFX6wniZAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(transaction_df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NXxFoQCmsHt",
        "outputId": "02459759-98f8-41f6-dcfd-19d8b4f5e0f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(24575, 23389)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#%% Step 4: Apply Apriori algorithm\n",
        "frequent_itemsets = apriori(transaction_df, min_support=0.004, use_colnames=True)\n",
        "# 0.004 ~ occur at least 100 (exacly 98.3) times out of a total of 24575 transactions"
      ],
      "metadata": {
        "id": "U_xzO5IUi0ZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(frequent_itemsets.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gglMhfAYpthV",
        "outputId": "b45e70d1-cef5-4e2b-a387-f33f4d031ab8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(421, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.9)\n",
        "selected_columns = ['antecedents', 'consequents', 'support', 'confidence']\n",
        "print(rules.loc[:, selected_columns])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ne8Uz3_ijh4e",
        "outputId": "7a881266-8040-4691-a2e6-8771c9fecd17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                antecedents       consequents   support  confidence\n",
            "0                  (carpet)             (red)  0.008057    0.994975\n",
            "1                   (gomez)          (selena)  0.006307    0.993590\n",
            "2              (housewives)            (real)  0.004924    0.952756\n",
            "3                  (markle)          (meghan)  0.009237    0.991266\n",
            "4                    (york)             (new)  0.005005    1.000000\n",
            "5                  (united)          (states)  0.009766    0.952381\n",
            "6                   (swift)          (taylor)  0.006633    0.964497\n",
            "7           (harry, markle)          (meghan)  0.004720    1.000000\n",
            "8           (harry, markle)          (prince)  0.004639    0.982759\n",
            "9           (harry, meghan)          (prince)  0.005738    0.940000\n",
            "10         (markle, prince)          (meghan)  0.005249    0.992308\n",
            "11  (harry, markle, meghan)          (prince)  0.004639    0.982759\n",
            "12  (harry, markle, prince)          (meghan)  0.004639    1.000000\n",
            "13          (harry, markle)  (meghan, prince)  0.004639    0.982759\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set max_rows to display all the results\n",
        "pd.options.display.max_rows = len(rules)\n",
        "\n",
        "# Print the association rules dataframe\n",
        "print(rules)"
      ],
      "metadata": {
        "id": "gDdQwUXKRrp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "antecedents_set = set(rules['antecedents'].tolist())\n",
        "consequents_set = set(rules['consequents'].tolist())\n",
        "combined_set = antecedents_set | consequents_set\n",
        "\n",
        "words = [list(word_set)[0] for word_set in combined_set]\n",
        "unique_words = set(words)\n",
        "print(unique_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUG11TjXvA0x",
        "outputId": "51fcfcc2-af4c-484a-d2ea-480f91364f8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'united', 'gomez', 'real', 'harry', 'prince', 'taylor', 'carpet', 'selena', 'meghan', 'new', 'states', 'swift', 'york', 'red', 'markle', 'housewives'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort the rules based on support in descending order\n",
        "sorted_rules = rules.sort_values('confidence', ascending=False)\n",
        "\n",
        "# Print the top 10 rules with highest support\n",
        "print(sorted_rules.head(20))"
      ],
      "metadata": {
        "id": "c40AqOMEo0kC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%%%%%%%%%%% Get only fake news %%%%%%%%%%%\n",
        "# Fig. 5\n",
        "\n",
        "onlyfake_l = dataset[dataset['label'] == 0]\n",
        "onlyfake_n = onlyfake_l[['statement']]\n",
        "\n",
        "# Step 1: Load sentences\n",
        "sentences_n = onlyfake_n['statement'].tolist()\n",
        "# Preprocessing steps here, e.g., tokenization, stopword removal, etc.\n",
        "\n",
        "# Step 2: Convert sentences to transactions\n",
        "transactions_n = []\n",
        "for sentence_n in sentences_n:\n",
        "    transaction_n = sentence_n.split()  # Split sentence into items (words)\n",
        "    transactions_n.append(transaction_n)\n",
        "\n",
        "# Step 3: Create transaction dataset\n",
        "te_n = TransactionEncoder()\n",
        "te_ary_n = te_n.fit_transform(transactions_n)\n",
        "transaction_df_n = pd.DataFrame(te_ary_n, columns=te_n.columns_)"
      ],
      "metadata": {
        "id": "fjII7QNJzGRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frequent_itemsets_n = apriori(transaction_df_n, min_support=0.002, use_colnames=True)"
      ],
      "metadata": {
        "id": "9hH1D21SzvTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(frequent_itemsets_n.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1J6hJEI0Kz1",
        "outputId": "41916bb1-7d25-493b-de1a-6eb1d6c20850"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1576, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rules_n = association_rules(frequent_itemsets_n, metric=\"confidence\", min_threshold=0.9)\n",
        "selected_columns = ['antecedents', 'consequents', 'support', 'confidence']\n",
        "print(rules_n.loc[:, selected_columns])"
      ],
      "metadata": {
        "id": "Ero6M8O6z2Lt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set max_rows to display all the results\n",
        "pd.options.display.max_rows = len(rules_n)\n",
        "\n",
        "# Print the association rules dataframe\n",
        "print(rules_n)"
      ],
      "metadata": {
        "id": "dgcBs-q-VtrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "antecedents_set_n = set(rules_n['antecedents'].tolist())\n",
        "consequents_set_n = set(rules_n['consequents'].tolist())\n",
        "combined_set_n = antecedents_set_n | consequents_set_n\n",
        "\n",
        "words_n = [list(word_set_n)[0] for word_set_n in combined_set_n]\n",
        "unique_words_n = set(words_n)\n",
        "print(unique_words_n)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqbUcAvP2QJ-",
        "outputId": "d2376d59-ce45-4464-d7bd-702d688a8724"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'nicole', 'jennifer', 'garner', 'gwen', 'scott', 'justin', 'jenner', 'kate', 'disick', 'health', 'ben', 'kendall', 'bieber', 'jamie', 'travis', 'pitt', 'stefani', 'selena', 'affleck', 'kidman', 'gomez', 'cyrus', 'holmes', 'swift', 'katie', 'harry', 'perry', 'kanye', 'meghan', 'robert', 'supreme', 'court', 'stewart', 'care', 'angelina', 'middleton', 'kim', 'united', 'lopez', 'jolie', 'kristen', 'aniston', 'states', 'walker', 'blake', 'foxx', 'west', 'prince', 'rodriguez', 'katy', 'brad', 'markle', 'theroux', 'shelton', 'pattinson', 'law', 'miley', 'kylie', 'taylor', 'kris', 'alex'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort the rules based on support in descending order\n",
        "sorted_rules = rules_n.sort_values('confidence', ascending=False)\n",
        "\n",
        "# Print the top 10 rules with highest support\n",
        "print(sorted_rules.head(20))"
      ],
      "metadata": {
        "id": "4nVyGapBwdex"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}